# Final year project - Explainable Artificial Intelligence for Multi-Sensor Information Fusion

For my final year project at Cardiff University, I had to create a multimodal video classifier with explainable AI capabilities. This was successfully done with the code in this repo having the capabilities to do the following:

* Train a model from scratch / from a previously trained model
* Perform classification on an input video with explainable AI capabilities using SAVR (previously VADR)
* Produce graphs showing the performance of the model

My final report can be view [here](https://pats.cs.cf.ac.uk/@archive_file?p=1370&n=final&f=1-report.pdf&SIG=8352ce3d25044eb9e4b49fb4df9b566d576bcbccacf41c07ae9e4e081496a672) which contains more detail. Overall this work was awarded 79%.

This was worked on alongside a new explanation technique called SAVR. Please see the reference at the bottom of the README for more.

# Example of output

![Clif diving](https://github.com/JackFurby/Final-year-project/blob/main/CliffDiving.gif)

![Hammering](https://github.com/JackFurby/Final-year-project/blob/main/Hammering2.gif)


# Reference

If you would like to use this work, please reference the original paper.

>H. Taylor, L. Hiley, J. Furby, A. Preece and D. Braines, "VADR: Discriminative Multimodal Explanations for Situational Understanding," 2020 IEEE 23rd International Conference on Information Fusion (FUSION), Rustenburg, South Africa, 2020, pp. 1-8, doi: 10.23919/FUSION45008.2020.9190215.




```
@INPROCEEDINGS{9190215,
  author={H. {Taylor} and L. {Hiley} and J. {Furby} and A. {Preece} and D. {Braines}}
  booktitle={2020 IEEE 23rd International Conference on Information Fusion (FUSION)},
  title={VADR: Discriminative Multimodal Explanations for Situational Understanding},
  year={2020},
  volume={},
  number={},
  pages={1-8},
  doi={10.23919/FUSION45008.2020.9190215}}
```
